* autograd是pytorch中神经网络的核心。

torch.Tensor的属性
====
属性|含义
----|----
.requires_grad|设置为True时，会开始跟踪针对tensor的所有操作。
.backward()|计算导数，如果tensor为标量（只包含一个元素数据），则不需要指定任何参数backward；<br>若其包含更多的元素，则需要指定gradient参数来指定张量的形状<br>完成计算后，可以调用.backward()来自动计算所有梯度。该张量的梯度将累积到.grad属性中。
.detach()|停止tensor历史记录的跟踪，可以使tensor和计算历史记录分离，并防止将来的计算被跟踪。
with torch.no_grad()|将代码块包装起来后，可使其停止跟踪历史记录。训练阶段requires_grad = True 的可训练参数有利于调参，在评估阶段我们不需要梯度。
Function|tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。每个张量都有一个 .<br>grad_fn 属性保存着创建了张量的 Function 的引用，（如果用户自己创建张量，则g rad_fn 是 None ）。

Example
=====
import torch<br>
x = torch.ones(2, 2, requires_grad=True)<br>
print(x)<br>
>>tensor([[1., 1.],<br>
        [1., 1.]], requires_grad=True)

y = x + 2   #y作为结果被创建，所以其有grad_fn<br>
print(y)<br>
>>
tensor([[3., 3.],<br>
        [3., 3.]], grad_fn=<AddBackward0>)<br>
print(y.grad_fn)<br>
>><br>
<AddBackward0 object at 0x7fe1db427470><br>

.requires_grad()会改变张量的requires_grad。如果没有提供相应的参数，则输入的标记默认为False
-----
Example<br>
a = torch.randn(2, 2)<br>
a = ((a * 3) / (a - 1))<br>
print(a.requires_grad)<br>
a.requires_grad_(True)<br>
print(a.requires_grad)<br>
b = (a * a).sum()<br>
print(b.grad_fn)<br>
>><br>
False<br>
True<br>
<SumBackward0 object at 0x7fe1db427dd8><br>

  
