* autograd是pytorch中神经网络的核心。
torch.Tensor的属性
====
属性|含义
----|----
.requires_grad|设置为True时，会开始跟踪针对tensor的所有操作。

.backward()|完成计算后，可以调用.backward()来自动计算所有梯度。该张量的梯度将累积到.grad属性中。

.detach()|停止tensor历史记录的跟踪，可以使tensor和计算历史记录分离，并防止将来的计算被跟踪。

with torch.no_grad()|将代码块包装起来后，可使其停止跟踪历史记录。训练阶段requires_grad = True 的可训练参数有利于调参，在评估阶段我们不需要梯度。
